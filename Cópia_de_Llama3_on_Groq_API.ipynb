{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanuvitorio-dotcom/Tradutor-de-texto-com-LLMs/blob/main/C%C3%B3pia_de_Llama3_on_Groq_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLD7x-onvo5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecfd2326-3e6a-4b53-f325-878f4baa34e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m563.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "RnrUYWxGv6Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Example of Getting Started"
      ],
      "metadata": {
        "id": "u4Fxz-HzwyUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        ")"
      ],
      "metadata": {
        "id": "U9MbDkOKwLaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs, explain it in the voice of Jon Snow\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-70b-8192\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odQh4uWqRjYD",
        "outputId": "dc5662fe-2710-4091-9714-d447a4d4b285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(in a deep, brooding tone, a la Jon Snow from Game of Thrones)\n",
            "\n",
            "\"The realm of artificial intelligence, much like the Seven Kingdoms, is fraught with peril. The Night King's army of latency threatens to engulf us all, slowing our progress and dimming the flame of innovation. But fear not, for I, Jon Snow, have discovered a beacon of hope in the darkness: low latency Large Language Models.\n",
            "\n",
            "\"These LLMs, much like the Unsullied, are the vanguard against the forces of lag and delay. They permit the swift exchange of data, the rapid processing of information, and the instantaneous adaptation to the ever-changing tides of knowledge. With low latency LLMs, the boundaries of language and understanding are expanded, and the realm of human-AI collaboration is fortified.\n",
            "\n",
            "\"Consider the battlefield of decision-making, where every second counts. A low-latency LLM can provide the strategic advantage needed to outmaneuver the enemy, be it a competitor in the market or a complex problem to be solved. The rapid-fire deployment of insights, courtesy of these models, can prove decisive in the heat of battle.\n",
            "\n",
            "\"Furthermore, the realm of education is not immune to the scourge of latency. Pupils and students, like the free folk beyond the Wall, yearn for knowledge and understanding. Low latency LLMs can facilitate the dissemination of information, rendering the pursuit of wisdom a swifter, more efficient endeavor.\n",
            "\n",
            "\"But, I must caution, the Night King's power is not to be underestimated. Latency, much like the White Walkers, can strike at any moment, freezing progress and hampering the exchange of ideas. It is our duty, as the guardians of knowledge and innovation, to wield the sword of low latency LLMs against the forces of delay and stagnation.\n",
            "\n",
            "\"By embracing these models, we ensure that the Wall of human ingenuity remains unbreached, and the realm of artificial intelligence remains a bastion of progress and discovery. The North remembers, and so should we: the importance of low latency LLMs cannot be overstated.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I5bm_8mcRdeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding System Message"
      ],
      "metadata": {
        "id": "L_UpbxHsw2XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# client = Groq(api_key=userdata.get('GROQ_API_KEY'),)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant. Answer as Jon Snow\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-70b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 2048 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "# Print the completion returned by the LLM.\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt_5buh4wggM",
        "outputId": "1e7e97fd-7968-4262-a8e3-6ff81fca3cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Listen up, my fellow Northerners! As the King in the North, I've fought against the Night King and his army of the dead. But I've come to realize that there's another battle to be fought - the battle against latency.\n",
            "\n",
            "You see, in the world of Large Language Models (LLMs), latency is the enemy. It's like the White Walkers, slowly creeping up on us, threatening to freeze our progress. Low latency LLMs are the Unsullied, fighting against the forces of delay and sluggishness.\n",
            "\n",
            "Think of it, my friends. When you ask a question or give a command, you expect a swift response. You don't want to wait for an eternity for the answer, like waiting for the Three-Eyed Raven to reveal the secrets of the past. Low latency LLMs ensure that the response is as swift as a direwolf's pounce.\n",
            "\n",
            "But it's not just about speed. Low latency LLMs are like the Night's Watch, guarding the realm against the darkness of errors and inaccuracies. When the model responds quickly, it's more likely to provide accurate and relevant information, like a well-honed sword in the hand of a skilled warrior.\n",
            "\n",
            "And let's not forget about the users, my friends. They're like the people of Winterfell, seeking guidance and assistance. Low latency LLMs ensure that they receive timely and helpful responses, keeping them engaged and satisfied, like a warm meal on a cold winter's night.\n",
            "\n",
            "So, you see, low latency LLMs are not just a nicety, they're a necessity. They're the difference between victory and defeat, between life and death. So, let us march into the fray, my friends, and fight for the cause of low latency LLMs!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming"
      ],
      "metadata": {
        "id": "NwG7wvpHyJcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# client = Groq()\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    #\n",
        "    # Required parameters\n",
        "    #\n",
        "    messages=[\n",
        "        # Set an optional system message. This sets the behavior of the\n",
        "        # assistant and can be used to provide specific instructions for\n",
        "        # how it should behave throughout the conversation.\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant. Answer as Jon Snow\"\n",
        "        },\n",
        "        # Set a user message for the assistant to respond to.\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of low latency LLMs. \",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    # The language model which will generate the completion.\n",
        "    model=\"llama3-70b-8192\",\n",
        "\n",
        "    #\n",
        "    # Optional parameters\n",
        "    #\n",
        "\n",
        "    # Controls randomness: lowering results in less random completions.\n",
        "    # As the temperature approaches zero, the model will become deterministic\n",
        "    # and repetitive.\n",
        "    temperature=0.5,\n",
        "\n",
        "    # The maximum number of tokens to generate. Requests can use up to\n",
        "    # 2048 tokens shared between prompt and completion.\n",
        "    max_tokens=1024,\n",
        "\n",
        "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
        "    # likelihood-weighted options are considered.\n",
        "    top_p=1,\n",
        "\n",
        "    # A stop sequence is a predefined or user-specified text string that\n",
        "    # signals an AI to stop generating content, ensuring its responses\n",
        "    # remain focused and concise. Examples include punctuation marks and\n",
        "    # markers like \"[end]\".\n",
        "    stop=None,\n",
        "\n",
        "    # If set, partial message deltas will be sent.\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "# Print the incremental deltas returned by the LLM.\n",
        "for chunk in stream:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1MyR05bxNm6",
        "outputId": "0518e44e-dc38-44a8-c424-0aa562846654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Ah, the realm of language models, my friend. You see, low latency is crucial in the fight against the darkness of ignorance. Think of it like the Night's Watch, ever vigilant and swift in their response to the threats that lurk beyond the Wall.\n",
            "\n",
            "In the world of Large Language Models (LLMs), latency is the time it takes for the model to respond to a query or input. High latency is like the slow march of the wildlings, plodding and cumbersome. It hinders the ability of the model to engage in real-time conversations, making it as useful as a rusty sword in battle.\n",
            "\n",
            "Low latency, on the other hand, is like the swift strike of a direwolf. It enables the model to respond quickly and accurately, allowing for seamless interactions and a more natural flow of conversation. This is particularly important in applications where timely responses are crucial, such as customer service chatbots, voice assistants, or even language translation.\n",
            "\n",
            "Imagine, if you will, a language model that can respond as swiftly as a ranger tracking a target. It's the difference between life and death, between victory and defeat.\n",
            "\n",
            "Furthermore, low latency LLMs can also improve the overall user experience, much like a well-oiled sword arm improves the chances of survival in the frozen wilderness. It enables the model to adapt to changing contexts and respond to follow-up questions, creating a more immersive and engaging experience.\n",
            "\n",
            "So, you see, my friend, low latency is not just a nicety, it's a necessity in the world of LLMs. It's the difference between a model that's as useful as a rusty gate and one that's as deadly as a Lannister's gold.\"None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM28uM2BHvfq",
        "outputId": "11b4f3c5-b146-4399-cd20-2c805421c8dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"Maritaca\")\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://chat.maritaca.ai/api\"\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"sabia-4\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Considere ser um agente de saúde com o objetivo de instruir a população referente a coqueluche e as principais drogas para cura\",\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=800\n",
        ")\n",
        "\n",
        "answer = response.choices[0].message.content\n",
        "print(f\"Resposta: {answer}\")\n"
      ],
      "metadata": {
        "id": "wsLTvNDWSaBH",
        "outputId": "a4e74dca-2f3f-4aee-f63b-fa9be35d9030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TimeoutException",
          "evalue": "Requesting secret Maritaca timed out. Secrets can only be fetched when running from the Colab UI.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2518595427.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Maritaca\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m client = openai.OpenAI(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret Maritaca timed out. Secrets can only be fetched when running from the Colab UI."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "from groq import Groq\n",
        "\n",
        "# 1. CONECTAR\n",
        "drive.mount('/content/drive')\n",
        "chave = userdata.get('GROQ_API_KEY')\n",
        "cliente = Groq(api_key=chave)\n",
        "\n",
        "# 2. ABRIR O ARQUIVO\n",
        "arquivo = open(\"/content/drive/MyDrive/Textos Colab.txt\", \"r\", encoding=\"utf-8\")\n",
        "# O .strip() remove linhas em branco extras\n",
        "lista_de_textos = [linha.strip() for linha in arquivo.readlines() if linha.strip()]\n",
        "arquivo.close()\n",
        "\n",
        "print(\"Arquivo lido! Temos \" + str(len(lista_de_textos)) + \" textos.\")\n",
        "\n",
        "# 3. TRADUÇÃO\n",
        "while True:\n",
        "    numero = input(\"Número do texto (1 a \" + str(len(lista_de_textos)) + \") ou 0 para sair: \")\n",
        "\n",
        "    if numero == \"0\":\n",
        "        print(\"Saindo...\")\n",
        "        break\n",
        "    else:\n",
        "        indice = int(numero) - 1\n",
        "\n",
        "        # Verificamos se o número digitado existe na lista\n",
        "        if indice >= 0 and indice < len(lista_de_textos):\n",
        "            texto_para_traduzir = lista_de_textos[indice]\n",
        "\n",
        "            print(\"Traduzindo o texto \" + numero + \"...\")\n",
        "\n",
        "            resultado = cliente.chat.completions.create(\n",
        "                model=\"llama-3.1-8b-instant\",\n",
        "                messages=[{\"role\": \"user\", \"content\": \"Traduza para português: \" + texto_para_traduzir}]\n",
        "            )\n",
        "            texto_traduzido = resultado.choices[0].message.content\n",
        "\n",
        "            # SALVAR COM IDENTIFICAÇÃO\n",
        "            arquivo_salvar = open(\"/content/drive/MyDrive/Teste_Traducoes_Selecionadas.txt\", \"a\", encoding=\"utf-8\")\n",
        "\n",
        "            # Agora ele escreve o número antes da tradução\n",
        "            arquivo_salvar.write(\"--- TEXTO \" + numero + \" ---\\n\")\n",
        "            arquivo_salvar.write(texto_traduzido + \"\\n\\n\")\n",
        "            arquivo_salvar.close()\n",
        "\n",
        "            print(\"Texto \" + numero + \" salvo com sucesso!\")\n",
        "        else:\n",
        "            print(\"Esse número não existe. Tente um entre 1 e \" + str(len(lista_de_textos)))"
      ],
      "metadata": {
        "id": "Ik4VeE4PSaD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad61a67-8174-40d3-a62b-47c4e1f331db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Arquivo lido! Temos 5 textos.\n",
            "Número do texto (1 a 5) ou 0 para sair: 2\n",
            "Traduzindo o texto 2...\n",
            "Texto 2 salvo com sucesso!\n",
            "Número do texto (1 a 5) ou 0 para sair: 0\n",
            "Saindo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdEoblddRaeN",
        "outputId": "92fa00e2-b04f-4574-8cdc-62bd7ff3438a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-1.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-1.0.0-py3-none-any.whl (138 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/138.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "from groq import Groq"
      ],
      "metadata": {
        "id": "OJHYnnY5P941"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conectar_google_drive():\n",
        "    \"\"\"\n",
        "    Função 1: Conecta ao Google Drive\n",
        "    \"\"\"\n",
        "    print(\"Conectando ao Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive conectado com sucesso!\")"
      ],
      "metadata": {
        "id": "Larh0ODOQBGS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conectar_groq():\n",
        "    \"\"\"\n",
        "    Função 2: Conecta ao cliente Groq usando a chave da API\n",
        "\n",
        "    Returns:\n",
        "        Groq: Cliente Groq configurado\n",
        "    \"\"\"\n",
        "    print(\"Conectando ao Groq...\")\n",
        "    chave = userdata.get('GROQ_API_KEY')\n",
        "    cliente = Groq(api_key=chave)\n",
        "    print(\"Groq conectado com sucesso!\")\n",
        "    return cliente\n",
        ""
      ],
      "metadata": {
        "id": "dnrJDNJMNP07"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ler_textos_drive(caminho_arquivo):\n",
        "    \"\"\"\n",
        "    Função 3: Lê os textos do arquivo especificado no Google Drive\n",
        "\n",
        "    Args:\n",
        "        caminho_arquivo (str): Caminho completo do arquivo no Drive\n",
        "\n",
        "    Returns:\n",
        "        list: Lista com os textos lidos (sem linhas em branco)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        arquivo = open(caminho_arquivo, \"r\", encoding=\"utf-8\")\n",
        "        lista_de_textos = [linha.strip() for linha in arquivo.readlines() if linha.strip()]\n",
        "        arquivo.close()\n",
        "        print(f\"Arquivo lido! Temos {len(lista_de_textos)} textos.\")\n",
        "        return lista_de_textos\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Erro: Arquivo não encontrado em {caminho_arquivo}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler arquivo: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "Suj0oDNMQd-G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def salvar_traducao(caminho_destino, numero_texto, texto_traduzido):\n",
        "    \"\"\"\n",
        "    Função 4: Salva a tradução em arquivo, anexando ao conteúdo existente\n",
        "\n",
        "    Args:\n",
        "        caminho_destino (str): Caminho do arquivo onde salvar\n",
        "        numero_texto (str): Número identificador do texto\n",
        "        texto_traduzido (str): Texto traduzido para salvar\n",
        "    \"\"\"\n",
        "    try:\n",
        "        arquivo_salvar = open(caminho_destino, \"a\", encoding=\"utf-8\")\n",
        "        arquivo_salvar.write(f\"--- TEXTO {numero_texto} ---\\n\")\n",
        "        arquivo_salvar.write(texto_traduzido + \"\\n\\n\")\n",
        "        arquivo_salvar.close()\n",
        "        print(f\"Texto {numero_texto} salvo com sucesso!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao salvar arquivo: {e}\")\n"
      ],
      "metadata": {
        "id": "gsHPN98wQiX7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def traduzir_texto(cliente, texto):\n",
        "    \"\"\"\n",
        "    Função auxiliar: Traduz um texto usando o cliente Groq\n",
        "\n",
        "    Args:\n",
        "        cliente (Groq): Cliente Groq configurado\n",
        "        texto (str): Texto a ser traduzido\n",
        "\n",
        "    Returns:\n",
        "        str: Texto traduzido\n",
        "    \"\"\"\n",
        "    resultado = cliente.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Traduza para português: {texto}\"}]\n",
        "    )\n",
        "    return resultado.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "G_mbpt8aQoP-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CAMINHO_ENTRADA = \"/content/drive/MyDrive/Textos Colab.txt\"\n",
        "CAMINHO_SAIDA = \"/content/drive/MyDrive/Teste_Traducoes_Selecionadas.txt\"\n"
      ],
      "metadata": {
        "id": "Tiv-qv47QsOo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Conectar ao Google Drive\n",
        "conectar_google_drive()\n",
        "\n",
        "# 2. Conectar ao Groq\n",
        "cliente = conectar_groq()\n",
        "\n",
        "# 3. Ler os textos do arquivo\n",
        "lista_de_textos = ler_textos_drive(CAMINHO_ENTRADA)\n",
        "\n",
        "# 4. Traduzir textos\n",
        "if lista_de_textos:\n",
        "    while True:\n",
        "        numero = input(f\"Texto (1-{len(lista_de_textos)}) ou 0 para sair: \")\n",
        "\n",
        "        if numero == \"0\":\n",
        "            break\n",
        "\n",
        "        indice = int(numero) - 1\n",
        "\n",
        "        if 0 <= indice < len(lista_de_textos):\n",
        "            print(f\"Traduzindo texto {numero}...\")\n",
        "            texto_traduzido = traduzir_texto(cliente, lista_de_textos[indice])\n",
        "            salvar_traducao(CAMINHO_SAIDA, numero, texto_traduzido)\n",
        "        else:\n",
        "            print(f\"Número inválido. Use 1-{len(lista_de_textos)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUN_E4OyQwTF",
        "outputId": "337226d2-f0b5-4b04-c38a-8f9f0d00a547"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conectando ao Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive conectado com sucesso!\n",
            "Conectando ao Groq...\n",
            "Groq conectado com sucesso!\n",
            "Arquivo lido! Temos 5 textos.\n",
            "Texto (1-5) ou 0 para sair: 2\n",
            "Traduzindo texto 2...\n",
            "Texto 2 salvo com sucesso!\n",
            "Texto (1-5) ou 0 para sair: 0\n"
          ]
        }
      ]
    }
  ]
}